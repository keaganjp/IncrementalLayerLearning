# -*- coding: utf-8 -*-
"""FFF_Conv_Cifar10_VGG11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x02BL-snu2HsbfFuXQavYQElyD4eoYeo
"""

import matplotlib.pyplot as plt
import torch
import time 

import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
from torch.optim import Adam
import torchvision.datasets
from torchvision.transforms import Compose, ToTensor, Normalize, Lambda
from torch.utils.data import DataLoader
import numpy as np
from math import floor

# The new version of FF - every layer has an untrainable predictor layer
# We train layer by layer

# The basic linear layer
class ILLLinearLayer(nn.Linear):
    def __init__(self, in_features, out_features, layer_lr = 0.005,
                 num_classes = 10, num_epochs = 1,
                 bias=True, device=None, dtype=None):
        super().__init__(in_features, out_features, bias, device, dtype)
        self.relu = torch.nn.ReLU()
        self.opt = Adam(self.parameters(), lr=layer_lr)
        self.num_epochs = num_epochs
        self.predictor = nn.Linear(out_features, num_classes, device=device)
        self.predictor.requires_grad = False
    
    def forward(self, x):
        layerOutput = self.relu(super().forward(x))
        predictorOutput = self.predictor.forward(layerOutput)
        return layerOutput, predictorOutput
    
    def predict(self, x):
        layerOutput, predictorOutput = self.forward(x)
        return F.softmax(predictorOutput, dim=1)
    

    # resnet skips - None or dict
    # if dict:
    # each element is output of i'th layer : input of j'th layer)
    def trainLayer(self, dataloader, previousLayers, device, writer, filename, resnet_skips = None):
        for epoch in range(self.num_epochs):
            criterion = nn.CrossEntropyLoss()
            start = time.time()
            for i, data in enumerate(dataloader):
                  originalInputs, labels = data
                  originalInputs = originalInputs.to(device)
                  labels = labels.to(device)
                  inputs = originalInputs
                  layer_counter = 0
                  skip_input = None
                  skip_target = None
                  for previous in previousLayers:
                    if isinstance(previous, nn.MaxPool2d) or isinstance(previous, nn.Flatten)  or isinstance(previous, nn.AvgPool2d) or isinstance(previous, nn.BatchNorm2d):
                        if skip_target is not None and skip_target == layer_counter:
                            inputs =  previous.forward(inputs+skip_input)
                        else:
                            inputs = previous.forward(inputs)
                    else:
                        if skip_target is not None and skip_target == layer_counter:
                            inputs,_ =  previous.forward(inputs+skip_input)
                        else:
                            inputs,_ = previous.forward(inputs)
                    if resnet_skips is not None:
                        if layer_counter in resnet_skips: # the current layer output is to be saved
                            skip_input = inputs
                    layer_counter +=1
                    
                  self.opt.zero_grad()
                  layerOutput, predictorOutput = self.forward(inputs)
                  layerLoss = criterion(predictorOutput, labels)
                  writer.add_scalar('Train/loss', layerLoss.item(), i)
                  # This is a local layer update, not a backprop through the net
                  layerLoss.backward()
                  self.opt.step()
            stop = time.time()
            #acc = eval_training(epoch)
            each_epoch_time = stop - start 
            with open(filename, 'a') as f:
                f.write('Train time for linear layer epoch {}: {:.3f}\n'.format(epoch, each_epoch_time))

# A convolutional Layer for FF

class ILLConv2D(nn.Conv2d):
    def __init__(self, in_channels, out_channels, kernel_size,
                 sampleInput,
                 stride=1, padding=1, dilation=1, groups=1, 
                 num_epochs = 1, layer_lr = 0.005, num_classes = 10,
                 bias=True, padding_mode='zeros', device=None, dtype=None):
        super().__init__(in_channels, out_channels, kernel_size, stride, padding, 
                        dilation, groups, bias, padding_mode, device, dtype)
        self.opt = Adam(self.parameters(), lr=layer_lr)
        self.num_epochs = num_epochs
        self.num_classes = num_classes
        self.device = device
        self.getandSetPredictorWeightShape(sampleInput, device)

    def getandSetPredictorWeightShape(self, sampleInput, device):
        convOutput = F.relu(super().forward(sampleInput.to(self.device)))
        self.predictor = nn.Linear(convOutput.shape[1]*convOutput.shape[2]*convOutput.shape[3],
                                   self.num_classes, 
                                   device=device)
        self.predictor.requires_grad = False
    
    def conv_output_shape(self, h_w, kernel_size=1, stride=1, pad=0, dilation=1):
        if type(kernel_size) is not tuple:
            kernel_size = (kernel_size, kernel_size)
        h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)
        w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)
        return h, w

    def forward(self, x):
        convOutput = F.relu(super().forward(x))
        predInput = convOutput.view(convOutput.size(0), -1)
        predOutput = self.predictor(predInput)
        return convOutput, predOutput
    
    def predict(self, x):
        layerOutput, predictorOutput = self.forward(x)
        return F.softmax(predictorOutput, dim=1)
    
    # resnet skips - None or dict
    # if dict:
    # each element is output of i'th layer : input of j'th layer)
    def trainLayer(self, dataloader, previousLayers, device, writer, filename, resnet_skips=None):
        for epoch in range(self.num_epochs):
            criterion = nn.CrossEntropyLoss()
            start = time.time()
            for i, data in enumerate(dataloader):
                  originalInputs, labels = data
                  originalInputs = originalInputs.to(device)
                  labels = labels.to(device)
                  inputs = originalInputs
                  layer_counter = 0
                  skip_input = None
                  skip_target = None
                  for previous in previousLayers:
                    if isinstance(previous, nn.MaxPool2d) or isinstance(previous, nn.Flatten)  or isinstance(previous, nn.AvgPool2d) or isinstance(previous, nn.BatchNorm2d):
                        if skip_target is not None and skip_target == layer_counter:
                            inputs =  previous.forward(inputs+skip_input)
                        else:
                            inputs = previous.forward(inputs)
                    else:
                        if skip_target is not None and skip_target == layer_counter:
                            inputs,_ =  previous.forward(inputs+skip_input)
                        else:
                            inputs,_ = previous.forward(inputs)
                    if resnet_skips is not None:
                        if layer_counter in resnet_skips: # the current layer output is to be saved
                            skip_input = inputs
                    layer_counter +=1
                  self.opt.zero_grad()
                  layerOutput, predictorOutput = self.forward(inputs)
                  layerLoss = criterion(predictorOutput, labels)
                  writer.add_scalar('Train/loss', layerLoss.item(), i)
                  # This is a local layer update, not a backprop through the net
                  layerLoss.backward()
                  self.opt.step()
            finish = time.time()
            each_epoch_time = finish - start 
            with open(filename, 'a') as f:
                f.write('Train time for conv layer epoch {}: {:.3f}\n'.format(epoch, each_epoch_time))
